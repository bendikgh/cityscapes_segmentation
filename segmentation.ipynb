{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from nets import SimpleSegmentationNet\n",
    "from data import CityscapesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Process ID: 2111490\n"
     ]
    }
   ],
   "source": [
    "# Print the current process ID\n",
    "print(\"Current Process ID:\", os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths = glob('/cluster/projects/vc/data/ad/open/Cityscapes/gtFine_trainvaltest/gtFine/train/bremen/**/*', recursive=True)\n",
    "train_seg_path = sorted(glob('/cluster/projects/vc/data/ad/open/Cityscapes/gtFine_trainvaltest/gtFine/train/*/*labelIds.png'))\n",
    "train_img_path = sorted(glob('/cluster/projects/vc/data/ad/open/Cityscapes/leftImg8bit_trainvaltest/leftImg8bit/train/*/*.png'))\n",
    "\n",
    "val_seg_path = sorted(glob('/cluster/projects/vc/data/ad/open/Cityscapes/gtFine_trainvaltest/gtFine/val/*/*labelIds.png'))\n",
    "val_img_path = sorted(glob('/cluster/projects/vc/data/ad/open/Cityscapes/leftImg8bit_trainvaltest/leftImg8bit/val/*/*.png'))\n",
    "\n",
    "test_seg_path = sorted(glob('/cluster/projects/vc/data/ad/open/Cityscapes/gtFine_trainvaltest/gtFine/test/bielefeld/*labelIds.png'))\n",
    "test_img_path = sorted(glob('/cluster/projects/vc/data/ad/open/Cityscapes/leftImg8bit_trainvaltest/leftImg8bit/test/bielefeld/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2975\n",
      "2975\n"
     ]
    }
   ],
   "source": [
    "print(len(train_seg_path))\n",
    "print(len(train_img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig,ax = plt.subplots(5,2,figsize=(10,30))\n",
    "for i in range(5):\n",
    "    img1 = plt.imread(test_seg_path[i])\n",
    "    img2 = plt.imread(test_img_path[i])\n",
    "    print(test_seg_path[i])\n",
    "    ax[i][0].imshow(img1)\n",
    "    ax[i][1].imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = CityscapesDataset(train_img_path, train_seg_path)\n",
    "valdata = CityscapesDataset(val_img_path, val_seg_path)\n",
    "# testdata = CityscapesDataset(test_img_path, test_seg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_loader = DataLoader(traindata, batch_size)\n",
    "val_loader = DataLoader(valdata, batch_size)\n",
    "# test_loader = DataLoader(testdata, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(data)\n",
    "print(data[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig,ax = plt.subplots(4,2,figsize=(10,30))\n",
    "for i in range(4):\n",
    "    img_raw = data[0][i].squeeze().permute((1, 2, 0))\n",
    "    img_labels = data[1][i].squeeze()\n",
    "    ax[i][0].imshow(img_raw)\n",
    "    ax[i][1].imshow(img_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labels import labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSegmentationNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (up1): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (up2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (out_conv): Conv2d(64, 35, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(labels)\n",
    "model = SimpleSegmentationNet(num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img, output, label):\n",
    "    img, output, label = img.cpu(), output.cpu(), label.cpu()\n",
    "    fig, ax = plt.subplots(len(img), 3, figsize=(15, 30))\n",
    "    cols = ['Input Image', 'Actual Output', 'Predicted Output']\n",
    "    \n",
    "    for x, col in zip(ax[0], cols):\n",
    "        x.set_title(col)\n",
    "\n",
    "    for i in range(len(img)):\n",
    "        Img = img[i].permute(1, 2, 0)\n",
    "        Lab = output[i].squeeze()  # Assuming output is a segmentation map\n",
    "        act = label[i].squeeze()  # Assuming label is a segmentation map\n",
    "        \n",
    "        ax[i][0].imshow(Img)\n",
    "        ax[i][1].imshow(act, cmap='tab20')  # Apply a colormap suitable for labels\n",
    "        ax[i][2].imshow(Lab, cmap='tab20')  # Apply the same colormap to predictions\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 30\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m trainloss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     10\u001b[0m     \u001b[39m# Training\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     img, label \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m     label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/.conda/envs/semseg/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/semseg/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/semseg/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/semseg/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/cityscapes_semantic_segmentation/data.py:31\u001b[0m, in \u001b[0;36mCityscapesDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m     30\u001b[0m     img \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mimread(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages_path[idx])\n\u001b[0;32m---> 31\u001b[0m     label \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39;49mimread(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels_path[idx])\n\u001b[1;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_img:\n\u001b[1;32m     34\u001b[0m         img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_img(img)\n",
      "File \u001b[0;32m~/.conda/envs/semseg/lib/python3.8/site-packages/matplotlib/pyplot.py:2195\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimread)\n\u001b[1;32m   2194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(fname, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 2195\u001b[0m     \u001b[39mreturn\u001b[39;00m matplotlib\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mimread(fname, \u001b[39mformat\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/semseg/lib/python3.8/site-packages/matplotlib/image.py:1563\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(parse\u001b[39m.\u001b[39murlparse(fname)\u001b[39m.\u001b[39mscheme) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1557\u001b[0m     \u001b[39m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1559\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease open the URL for reading and pass the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1560\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresult to Pillow, e.g. with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1561\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1562\u001b[0m         )\n\u001b[0;32m-> 1563\u001b[0m \u001b[39mwith\u001b[39;00m img_open(fname) \u001b[39mas\u001b[39;00m image:\n\u001b[1;32m   1564\u001b[0m     \u001b[39mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[1;32m   1565\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mPngImagePlugin\u001b[39m.\u001b[39mPngImageFile) \u001b[39melse\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m             pil_to_array(image))\n",
      "File \u001b[0;32m~/.conda/envs/semseg/lib/python3.8/site-packages/PIL/ImageFile.py:105\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodermaxblock \u001b[39m=\u001b[39m MAXBLOCK\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m is_path(fp):\n\u001b[1;32m    104\u001b[0m     \u001b[39m# filename\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(fp, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m fp\n\u001b[1;32m    107\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"checkpoints/cnn\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    trainloss = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        # Training\n",
    "        img, label = data[0].to(device), data[1].to(device)\n",
    "        label = label.squeeze(1).to(dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(img)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        trainloss += loss.item()\n",
    "\n",
    "    train_loss.append(trainloss / len(train_loader))    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valloss = 0\n",
    "        total_correct = 0\n",
    "        total_pixels = 0\n",
    "        \n",
    "        for data in val_loader:\n",
    "            # Validation\n",
    "            img, label = data[0].to(device), data[1].to(device)\n",
    "            label = label.squeeze(1).to(dtype=torch.long)\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "            valloss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_correct += (predicted == label).sum().item()\n",
    "            total_pixels += label.nelement()\n",
    "\n",
    "        # show(img, predicted.unsqueeze(1), label)\n",
    "\n",
    "    val_loss.append(valloss / len(val_loader))\n",
    "    val_accuracy.append(total_correct / total_pixels)\n",
    "    \n",
    "    print(\"Epoch: {} , Train Loss: {} , Valid Loss: {} , Valid Acc: {:.2f}%\".format(i, train_loss[-1], val_loss[-1], 100 * val_accuracy[-1]))\n",
    "\n",
    "    if i%20 == 0:\n",
    "         # Checkpointing\n",
    "        checkpoint = {\n",
    "            'epoch': i,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, f'checkpoint_epoch_{i}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting the Training VS Validation Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x146616b8b970>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss,color='b',label='train loss')\n",
    "plt.plot(val_loss,color='r',label = 'val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_time = 0\n",
    "    \n",
    "    for data in test_loader:\n",
    "        image, label = data[0].to(device), data[1].to(device)\n",
    "        label = label.squeeze(1).to(dtype=torch.long)\n",
    "\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        output = model(image)\n",
    "        \n",
    "        # End timer\n",
    "        end_time = time.time()\n",
    "        total_time += end_time - start_time\n",
    "\n",
    "        # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.nelement()\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('Accuracy of the model on the test images: {:.2f}%'.format(accuracy))\n",
    "\n",
    "# Calculate FPS\n",
    "# Here, 'total' is the total number of images processed\n",
    "fps = total / total_time\n",
    "print('Inference speed: {:.2f} FPS'.format(fps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c = 0\n",
    "with torch.no_grad():\n",
    "    for img,label in (test_loader):\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(img)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            show(img, predicted.unsqueeze(1), label)\n",
    "\n",
    "            if c>5:\n",
    "                break\n",
    "            c+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18 (default, Sep 11 2023, 13:40:15) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55f208f0ea97f7cf4c595e1c29c94907e19cd343c332570dca0f403dd8f88931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
