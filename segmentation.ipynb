{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from nets import SimpleSegmentationNet\n",
    "from data import CityscapesDataset\n",
    "from labels import training_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the current process ID\n",
    "print(\"Current Process ID:\", os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths = glob('/cluster/projects/vc/data/ad/open/Cityscapes/gtFine_trainvaltest/gtFine/train/bremen/**/*', recursive=True)\n",
    "train_seg_path = sorted(glob('/cluster/projects/vc/data/ad/open/Cityscapes/gtFine_trainvaltest/gtFine/train/*/*labelIds.png'))\n",
    "train_img_path = sorted(glob('/cluster/projects/vc/data/ad/open/Cityscapes/leftImg8bit_trainvaltest/leftImg8bit/train/*/*.png'))\n",
    "\n",
    "val_seg_path = sorted(glob('/cluster/projects/vc/data/ad/open/Cityscapes/gtFine_trainvaltest/gtFine/val/*/*labelIds.png'))\n",
    "val_img_path = sorted(glob('/cluster/projects/vc/data/ad/open/Cityscapes/leftImg8bit_trainvaltest/leftImg8bit/val/*/*.png'))\n",
    "\n",
    "test_seg_path = sorted(glob('/cluster/projects/vc/data/ad/open/Cityscapes/gtFine_trainvaltest/gtFine/test/bielefeld/*labelIds.png'))\n",
    "test_img_path = sorted(glob('/cluster/projects/vc/data/ad/open/Cityscapes/leftImg8bit_trainvaltest/leftImg8bit/test/bielefeld/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_seg_path))\n",
    "print(len(train_img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig,ax = plt.subplots(5,2,figsize=(10,30))\n",
    "for i in range(5):\n",
    "    img1 = plt.imread(test_seg_path[i])\n",
    "    img2 = plt.imread(test_img_path[i])\n",
    "    print(test_seg_path[i])\n",
    "    ax[i][0].imshow(img1)\n",
    "    ax[i][1].imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = CityscapesDataset(train_img_path, train_seg_path)\n",
    "valdata = CityscapesDataset(val_img_path, val_seg_path)\n",
    "# testdata = CityscapesDataset(test_img_path, test_seg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_loader = DataLoader(traindata, batch_size)\n",
    "val_loader = DataLoader(valdata, batch_size)\n",
    "# test_loader = DataLoader(testdata, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(data)\n",
    "print(data[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig,ax = plt.subplots(4,2,figsize=(10,30))\n",
    "for i in range(4):\n",
    "    img_raw = data[0][i].squeeze().permute((1, 2, 0))\n",
    "    img_labels = data[1][i].squeeze()\n",
    "    ax[i][0].imshow(img_raw)\n",
    "    ax[i][1].imshow(img_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = training_classes\n",
    "model = SimpleSegmentationNet(num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img, output, label):\n",
    "    img, output, label = img.cpu(), output.cpu(), label.cpu()\n",
    "    fig, ax = plt.subplots(len(img), 3, figsize=(15, 30))\n",
    "    cols = ['Input Image', 'Actual Output', 'Predicted Output']\n",
    "    \n",
    "    for x, col in zip(ax[0], cols):\n",
    "        x.set_title(col)\n",
    "\n",
    "    for i in range(len(img)):\n",
    "        Img = img[i].permute(1, 2, 0)\n",
    "        Lab = output[i].squeeze()  # Assuming output is a segmentation map\n",
    "        act = label[i].squeeze()  # Assuming label is a segmentation map\n",
    "        \n",
    "        ax[i][0].imshow(Img)\n",
    "        ax[i][1].imshow(act, cmap='tab20')  # Apply a colormap suitable for labels\n",
    "        ax[i][2].imshow(Lab, cmap='tab20')  # Apply the same colormap to predictions\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 30\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint_dir = \"checkpoints/cnn\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    trainloss = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        # Training\n",
    "        img, label = data[0].to(device), data[1].to(device)\n",
    "        label = label.squeeze(1).to(dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(img)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        trainloss += loss.item()\n",
    "\n",
    "    train_loss.append(trainloss / len(train_loader))    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valloss = 0\n",
    "        total_correct = 0\n",
    "        total_pixels = 0\n",
    "        \n",
    "        for data in val_loader:\n",
    "            # Validation\n",
    "            img, label = data[0].to(device), data[1].to(device)\n",
    "            label = label.squeeze(1).to(dtype=torch.long)\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "            valloss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_correct += (predicted == label).sum().item()\n",
    "            total_pixels += label.nelement()\n",
    "\n",
    "        # show(img, predicted.unsqueeze(1), label)\n",
    "\n",
    "    val_loss.append(valloss / len(val_loader))\n",
    "    val_accuracy.append(total_correct / total_pixels)\n",
    "    \n",
    "    print(\"Epoch: {} , Train Loss: {} , Valid Loss: {} , Valid Acc: {:.2f}%\".format(i, train_loss[-1], val_loss[-1], 100 * val_accuracy[-1]))\n",
    "\n",
    "    if i%20 == 0:\n",
    "         # Checkpointing\n",
    "        checkpoint = {\n",
    "            'epoch': i,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, f'checkpoint_epoch_{i}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = training_classes\n",
    "model = SimpleSegmentationNet(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "checkpoint_dir = \"checkpoints/cnn\"\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_15.pth') # Replace X with the epoch number\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Update model and optimizer states\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# If you need to resume training from a specific epoch\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "# If you also need to access the loss and accuracy history\n",
    "train_loss = checkpoint['train_loss']\n",
    "val_loss = checkpoint['val_loss']\n",
    "val_accuracy = checkpoint['val_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting the Training VS Validation Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss,color='b',label='train loss')\n",
    "plt.plot(val_loss,color='r',label = 'val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_images_processed = 0  # Initialize total images processed\n",
    "    \n",
    "    for data in tqdm(val_loader):\n",
    "        image, label = data[0].to(device), data[1].to(device)\n",
    "        label = label.squeeze(1).to(dtype=torch.long)\n",
    "\n",
    "        batch_size = label.size(0)  # Get batch size\n",
    "        total_images_processed += batch_size  # Accumulate total images processed\n",
    "        output = model(image)\n",
    "\n",
    "        # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.nelement()\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('Accuracy of the model on the test images: {:.2f}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for data in valdata:\n",
    "    image, _ = data[0].to(device), data[1].to(device)\n",
    "    data_list.append(image)\n",
    "\n",
    "\n",
    "model.eval()  \n",
    "\n",
    "start_time = time.time() \n",
    "\n",
    "for i in range(len(data_list)):\n",
    "    output = model(data_list[i])\n",
    "\n",
    "end_time = time.time()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = end_time - start_time  # Total time for inference\n",
    "fps = len(data_list) / total_time  # Calculate FPS\n",
    "\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "# Total inference time: 5.27 seconds\n",
    "# FPS: 94.91\n",
    "\n",
    "\n",
    "print(f\"Total inference time: {total_time:.2f} seconds\")\n",
    "print(f\"FPS: {fps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "with torch.no_grad():\n",
    "    for img,label in (val_loader):\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(img)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            show(img, predicted.unsqueeze(1), label)\n",
    "\n",
    "            if c>5:\n",
    "                break\n",
    "            c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_paths = glob(\"/cluster/home/bendikgh/cityscapes_semantic_segmentation/trondheim_images/*\")\n",
    "print(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "\n",
    "def center_crop_to_aspect_ratio(img):\n",
    "    original_width, original_height = img.size   # Get dimensions\n",
    "\n",
    "    # Determine the target dimensions based on the desired 2:1 width to height ratio\n",
    "    # The limiting dimension will dictate the size of the crop\n",
    "    if original_width >= 2 * original_height:\n",
    "        # The height is the limiting dimension, so the width will be twice the height\n",
    "        target_height = original_height\n",
    "        target_width = 2 * original_height\n",
    "    else:\n",
    "        # The width is the limiting dimension, so the height will be half the width\n",
    "        target_width = original_width\n",
    "        target_height = original_width / 2\n",
    "\n",
    "    left = (original_width - target_width) / 2\n",
    "    top = (original_height - target_height) / 2\n",
    "    right = (original_width + target_width) / 2\n",
    "    bottom = (original_height + target_height) / 2\n",
    "\n",
    "    # Crop the center of the image to the target size\n",
    "    img = img.crop((left, top, right, bottom))\n",
    "    return img\n",
    "\n",
    "# Replace this with the transformations used during your model's training\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process(img):\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = center_crop_to_aspect_ratio(img)\n",
    "    img = transform.Resize((1024, 2048))(img)\n",
    "    img = transform.ToTensor()(img)\n",
    "    return img\n",
    "\n",
    "target_width, target_height = 2048, 1024\n",
    "images = [process(Image.open(img_path)) for img_path in image_paths]\n",
    "\n",
    "img_val = next(iter(val_loader))[0][0]\n",
    "images.append(img_val)\n",
    "\n",
    "print(len(images))\n",
    "# print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for i in range(len(images)):\n",
    "        img = images[i].unsqueeze(0).to(device)\n",
    "\n",
    "        outputs = model(img)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        print(img.size())\n",
    "        print(predicted.size())\n",
    "\n",
    "        show(img, predicted.unsqueeze(1), predicted.unsqueeze(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55f208f0ea97f7cf4c595e1c29c94907e19cd343c332570dca0f403dd8f88931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
